# South African Bank Risk Dataset

[![DOI](https://zenodo.org/badge/358567011.svg)](https://zenodo.org/badge/latestdoi/358567011)

Give Feedback ðŸ“‘: [DSFSI Resource Feedback Form](https://docs.google.com/forms/d/e/1FAIpQLSf7S36dyAUPx2egmXbFpnTBuzoRulhL5Elu-N1eoMhaO7v10w/formResponse)

## About Dataset

This repository is an initial pipeline for reading, processing, labelling and classifying unstructured annual reports of South African (SA) banks with the aim of identifying financial risk. It leveraged work by the Corporate Financial Information Environment-Final Report Structure Extractor (CFIEâ€“FRSE) of El-Haj et al. which created a corpus of annual reports of United Kingdom (UK) companies.

## About Data collection methodology

A register of banks licensed in SA was used to download annual reports from company websites and online portals.  Company structures, trading practices
and branding complicated the collection. The South African Bank of Athens Limited (â€˜SABAâ€™) is an example, trading as Grobank after acquisition by GroCapital Holdings Proprietary Limited (â€˜GroCapital Holdingsâ€™). Subsidiary and group companies were analysed as separate entities. This applied to ABSA, Investec, Nedbank and Standard Bank.
A UK report was selected as reference to validate pre-processing results against the CFIEâ€“FRSE tool. Pre-porcessing also mapped reports to companies and years, and labeled reports as positive or negative with respect to risk. Data was prepared by extracting text, counting all words and those in wordlists, and validating results against the reference. Processing included Bag of Words, word embedding, feature scaling and topic analysis. Modelling entailed various classifiers for which results were compared and the best performing models were applied in a prediction use case.

## Description of the data

Of the potential 297 annual reports from 2009 to 2019, 258 were sourced with the balance not found or the bank was not operational that year. Only 7 reports comprised multiple documents so for simplicity each document was treated as a report.

## Repository Organisation
------------
    â”œâ”€â”€ data
    â”‚   â”œâ”€â”€ interim
    â”‚   â”‚   â”œâ”€â”€ wordlists                     <- Lists of keywords/substrings to extract linguistic features
    â”‚   â”‚   â”‚   â”œâ”€â”€ causal.txt                <- Causal reasoning wordlist relating to performance commentary, based on a composite wordlist from El-Haj et al.
    â”‚   â”‚   â”‚   â”œâ”€â”€ causalMartin50.txt        <- Causal reasoning wordlist based on El-Haj et al.
    â”‚   â”‚   â”‚   â”œâ”€â”€ causalMartinAll.txt       <- Causal reasoning wordlist based on El-Haj et al.
    â”‚   â”‚   â”‚   â”œâ”€â”€ ForwardLooking.txt        <- Forwardlooking wordlist based on the list proposed by Hussainey et al.
    â”‚   â”‚   â”‚   â”œâ”€â”€ forwardLookingNew.txt     <- Forwardlooking wordlist based on an updated version of the list proposed by Hussainey et al.
    â”‚   â”‚   â”‚   â”œâ”€â”€ HenryNeg2006.txt          <- Negative wordlist based on Henry (2006)
    â”‚   â”‚   â”‚   â”œâ”€â”€ HenryNeg2008.txt          <- Negative wordlist based on Henry (2008)
    â”‚   â”‚   â”‚   â”œâ”€â”€ HenryPos2006.txt          <- Positive wordlist based on Henry (2006)
    â”‚   â”‚   â”‚   â”œâ”€â”€ HenryPos2008.txt          <- Positive wordlist based on Henry (2008)
    â”‚   â”‚   â”‚   â”œâ”€â”€ LMnegative.txt            <- Negative wordlist based on Loughran and McDonald
    â”‚   â”‚   â”‚   â”œâ”€â”€ LMpositive.txt            <- Positive wordlist based on Loughran and McDonald
    â”‚   â”‚   â”‚   â”œâ”€â”€ newStrategy.txt           <- Wordlist for identifying strategy-related commentary based on El-Haj et al.
    â”‚   â”‚   â”‚   â”œâ”€â”€ performance.txt           <- Wordlist for identifying performance-related commentary based on El-Haj et al.
    â”‚   â”‚   â”‚   â””â”€â”€ Uncertainty.txt           <- Uncertainty wordlist based on Loughran and McDonald
    â”‚   â”‚   â”œâ”€â”€ coMap.csv                     <- Reference data: Company mapping file
    â”‚   â”‚   â”œâ”€â”€ docMap.csv                    <- Reference data: Document mapping file 
    â”‚   â”‚   â”œâ”€â”€ fileNoMap.csv                 <- Output data: PDF files read that were not in the document mapping file 
    â”‚   â”‚   â”œâ”€â”€ header.csv                    <- Output data: Header text extracted from PDF files
    â”‚   â”‚   â”œâ”€â”€ match.csv                     <- Keywords to match and classify headers based on El-Haj et al.
    â”‚   â”‚   â””â”€â”€ MatchToC.txt                  <- Keywords to identify Table of Content header
    â”‚   â”œâ”€â”€ processed
    â”‚   â”‚   â”œâ”€â”€ docReadability.csv            <- Readability results per document without the text
    â”‚   â”‚   â”œâ”€â”€ pageBlocks.csv                <- Text blocks per page per document with word counts per page and per wordlist
    â”‚   â”‚   â”œâ”€â”€ pageText.csv                  <- Text per page per document with word counts per page and per wordlist
    â”‚   â”‚   â”œâ”€â”€ pageTextRef.csv               <- Text per page of reference report to validate word counts with CFIEâ€“FRSE
    â”‚   â”‚   â”œâ”€â”€ prob_test_LR.csv              <- Probability of risk on test dataset reports predicted by Logistic Regression
    â”‚   â”‚   â””â”€â”€ prob_test_SVMa.csv            <- Probability of risk on test dataset reports predicted by Support Vector Machine with auto gamma
    â”‚   â””â”€â”€ raw
    â”‚       â”œâ”€â”€ Annual Reports                <- PDF files downloaded from internet websites
    â”‚       â”‚   â”œâ”€â”€ Bank                      <- Annual reports read and processed to create the dataset
    â”‚       â”‚   â””â”€â”€ Insurer                   <- Empty folder for insurer reports to be stored in future
    â”‚       â””â”€â”€ Other                         <- Other risk-related reports downloaded from internet websites
    â”œâ”€â”€ notebooks                                       <- Python code
    â”‚   â”œâ”€â”€ colab                                       <- Code for Google Colaboratory and cloud runtime
    â”‚   â”‚   â”œâ”€â”€ 1_0_Colab Import.ipynb                  <- Extract PDF text, convert booklets, count words by page and write pageText.csv
    â”‚   â”‚   â”œâ”€â”€ 2_0_Colab EDA.ipynb                     <- Exploratory Data Analysis (incl. Chi-Square) and write docReadability.csv
    â”‚   â”‚   â”œâ”€â”€ 3_0_Colab Classifier.ipynb              <- Loop through raw and stemmed/lemmatized tokens as well as classifiers
    â”‚   â”‚   â”œâ”€â”€ 4_0_Colab Classifier Applied LR.ipynb   <- Logistic Regression prediction, LIME, feature selection and write prob_test_LR.csv
    â”‚   â”‚   â””â”€â”€ 4_1_Colab Classifier Applied SVMa.ipynb <- Support Vector Machine (with auto gamma) prediction, LIME, feature selection and write prob_test_SVMa.csv
    â”‚   â””â”€â”€ jupyter                                     <- Code for Python 3 and local runtime e.g. using Jupyter or JupyterLab
    â”‚       â”‚â”€â”€ 1_0_Import.ipynb                        <- Extract PDF text, convert booklets, count words by page and write pageText.csv
    â”‚       â””â”€â”€ 1_1_Import Count Blocks.ipynb           <- Extract PDF text, convert booklets, count words by page and write pageBlocks.csv
    â”œâ”€â”€ .gitignore
    â”œâ”€â”€ LICENSE
    â””â”€â”€ README.md

## Online Repository link

* [Zenodo Data Repository](https://zenodo.org/record/6769996) - Link to the data repository.

## Authors

* **Lamont Theron** 
* **Vukosi Marivate** - [@vukosi](https://twitter.com/vukosi)

See also the list of [contributors](https://github.com/dsfsi/za-bank-risk/contributors) who participated in this project.

## License

Data is Licensed under CC 4.0 BY SA
Code is Licences under MIT License.
